{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ScoreCard Analysis Notebook\n",
    "\n",
    "This notebook loads pickled pipeline state and performs comprehensive analysis including:\n",
    "- Model validation metrics\n",
    "- Prediction analysis\n",
    "- GPT justification review\n",
    "- Color change detection\n",
    "\n",
    "**Prerequisites:** Run `scorecard_demo-2.ipynb` first to generate the state, then pickle it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 0. Save State (Run in Original Notebook First)\n",
    "\n",
    "**Copy this cell to your source notebook and run it to save state:**\n",
    "\n",
    "```python\n",
    "from scorecard import save_state\n",
    "\n",
    "# Save the full pipeline state\n",
    "save_state(\n",
    "    state=state,\n",
    "    pipeline=pipeline,\n",
    "    rag=rag,\n",
    "    config=config,\n",
    "    conn=conn,\n",
    "    path=\"./pipeline_state.pkl\",\n",
    "    include_models=True,\n",
    "    verbose=True,\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Load Pickled State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from scorecard import load_state\n",
    "\n",
    "# Load the saved state\n",
    "# Set reload_embeddings=True if you need to run GPT justifications\n",
    "state, pipeline, rag, config, conn = load_state(\n",
    "    path=\"./pipeline_state.pkl\",\n",
    "    reconnect=True,           # Re-establish ES/SQL/GPT connections\n",
    "    reload_nlp=False,         # Skip spaCy (not needed for analysis)\n",
    "    reload_embeddings=True,   # Needed for RAG/GPT operations\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick verification of loaded state\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"LOADED STATE VERIFICATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nDataFrames:\")\n",
    "for name in ['enriched_df', 'predictions_df', 'complete_df']:\n",
    "    df = getattr(state, name, None)\n",
    "    if df is not None:\n",
    "        print(f\"  - {name}: {df.shape[0]:,} rows x {df.shape[1]} cols\")\n",
    "    else:\n",
    "        print(f\"  - {name}: None\")\n",
    "\n",
    "print(f\"\\nModel Horizons:\")\n",
    "for h, key in state.best_model_key_by_horizon.items():\n",
    "    print(f\"  - H{h}: {key[:60]}...\")\n",
    "\n",
    "print(f\"\\nPredictions by Horizon:\")\n",
    "for h, df in state.predictions_df_by_horizon.items():\n",
    "    if df is not None:\n",
    "        print(f\"  - H{h}: {df.shape[0]:,} predictions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Imports for Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from IPython.display import display, HTML, Markdown\n",
    "\n",
    "from scorecard import (\n",
    "    # Reporting\n",
    "    enrich_for_reporting,\n",
    "    generate_summary_tables,\n",
    "    plot_prediction_dashboard,\n",
    "    display_one_note,\n",
    "    display_flagged_notes,\n",
    "    generate_flagged_report,\n",
    "    # Model validation\n",
    "    compute_baseline_metrics,\n",
    "    plot_baseline_comparison,\n",
    "    compute_calibration_metrics,\n",
    "    plot_calibration_curves,\n",
    "    plot_precision_recall_curves,\n",
    "    analyze_temporal_performance,\n",
    "    plot_temporal_performance,\n",
    "    analyze_errors,\n",
    "    plot_error_analysis,\n",
    "    extract_feature_importance,\n",
    "    plot_feature_importance,\n",
    "    generate_word_clouds,\n",
    "    # RAG\n",
    "    ScoreCardRag,\n",
    ")\n",
    "\n",
    "# Plot settings\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 10\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "print(\"All imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Confusion Matrix Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"CONFUSION MATRIX ANALYSIS (Test Set)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "color_labels = [\"Green\", \"Yellow\", \"Red\"]\n",
    "\n",
    "for h_int, model_dict in state.best_model_by_horizon.items():\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"H{h_int} - {state.best_model_key_by_horizon.get(h_int, 'Unknown')[:50]}...\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    y_test = model_dict.get(\"y_test\")\n",
    "    y_pred = model_dict.get(\"y_pred\")\n",
    "    \n",
    "    if y_test is None or y_pred is None:\n",
    "        print(\"  No test data available\")\n",
    "        continue\n",
    "    \n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    cm_norm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Raw counts\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=color_labels, yticklabels=color_labels, ax=axes[0])\n",
    "    axes[0].set_xlabel('Predicted')\n",
    "    axes[0].set_ylabel('Actual')\n",
    "    axes[0].set_title(f'H{h_int} Confusion Matrix (Counts)')\n",
    "    \n",
    "    # Normalized\n",
    "    sns.heatmap(cm_norm, annot=True, fmt='.2%', cmap='Blues',\n",
    "                xticklabels=color_labels, yticklabels=color_labels, ax=axes[1])\n",
    "    axes[1].set_xlabel('Predicted')\n",
    "    axes[1].set_ylabel('Actual')\n",
    "    axes[1].set_title(f'H{h_int} Confusion Matrix (Normalized)')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Classification report\n",
    "    print(f\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred, target_names=color_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Baseline Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"BASELINE COMPARISON\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\\nComparing our model against naive baselines:\")\n",
    "print(\"  - Most Frequent: Always predict the majority class\")\n",
    "print(\"  - Last Color: Predict the same color as previous note\")\n",
    "print(\"  - Stratified: Random prediction based on class distribution\")\n",
    "\n",
    "for h_int, model_dict in state.best_model_by_horizon.items():\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"H{h_int} Baseline Comparison\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    y_test = model_dict.get(\"y_test\")\n",
    "    y_pred = model_dict.get(\"y_pred\")\n",
    "    y_proba = model_dict.get(\"y_proba\")\n",
    "    \n",
    "    if y_test is None or y_pred is None:\n",
    "        print(\"  No test data available\")\n",
    "        continue\n",
    "    \n",
    "    # Build temp dataframe for baseline comparison\n",
    "    df_temp = pd.DataFrame({\n",
    "        'true': y_test,\n",
    "        'pred': y_pred,\n",
    "    })\n",
    "    \n",
    "    if y_proba is not None:\n",
    "        df_temp['prob_0'] = y_proba[:, 0]\n",
    "        df_temp['prob_1'] = y_proba[:, 1] if y_proba.shape[1] > 1 else 0\n",
    "        df_temp['prob_2'] = y_proba[:, 2] if y_proba.shape[1] > 2 else 0\n",
    "    \n",
    "    # Compute baseline metrics\n",
    "    try:\n",
    "        baseline_metrics = compute_baseline_metrics(\n",
    "            df_temp, \n",
    "            true_col='true', \n",
    "            pred_col='pred',\n",
    "            prob_cols=['prob_0', 'prob_1', 'prob_2'] if y_proba is not None else None\n",
    "        )\n",
    "        \n",
    "        print(f\"\\nModel vs Baselines:\")\n",
    "        for name, metrics in baseline_metrics.items():\n",
    "            acc = metrics.get('accuracy', 0)\n",
    "            f1 = metrics.get('f1_macro', 0)\n",
    "            print(f\"  {name:20s}: Accuracy={acc:.3f}, F1-macro={f1:.3f}\")\n",
    "        \n",
    "        # Plot comparison\n",
    "        fig = plot_baseline_comparison(baseline_metrics)\n",
    "        fig.suptitle(f\"H{h_int} - Model vs Baselines\", fontsize=14, fontweight='bold', y=1.02)\n",
    "        plt.show()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  Error computing baselines: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Calibration Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"CALIBRATION ANALYSIS\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\\nWhen the model says '80% confident', is it right 80% of the time?\")\n",
    "\n",
    "for h_int, model_dict in state.best_model_by_horizon.items():\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"H{h_int} Calibration\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    y_test = model_dict.get(\"y_test\")\n",
    "    y_proba = model_dict.get(\"y_proba\")\n",
    "    \n",
    "    if y_test is None or y_proba is None:\n",
    "        print(\"  No probability data available\")\n",
    "        continue\n",
    "    \n",
    "    try:\n",
    "        # Build temp dataframe\n",
    "        df_temp = pd.DataFrame({\n",
    "            'true': y_test,\n",
    "            'prob_0': y_proba[:, 0],\n",
    "            'prob_1': y_proba[:, 1] if y_proba.shape[1] > 1 else 0,\n",
    "            'prob_2': y_proba[:, 2] if y_proba.shape[1] > 2 else 0,\n",
    "        })\n",
    "        \n",
    "        # Compute calibration metrics\n",
    "        cal_metrics = compute_calibration_metrics(\n",
    "            df_temp, \n",
    "            true_col='true',\n",
    "            prob_cols=['prob_0', 'prob_1', 'prob_2']\n",
    "        )\n",
    "        \n",
    "        print(f\"\\nCalibration Metrics per Class:\")\n",
    "        for cls, metrics in cal_metrics.items():\n",
    "            ece = metrics.get('expected_calibration_error', 0)\n",
    "            brier = metrics.get('brier_score', 0)\n",
    "            print(f\"  Class {cls}: ECE={ece:.4f}, Brier={brier:.4f}\")\n",
    "        \n",
    "        # Plot calibration curves\n",
    "        fig = plot_calibration_curves(df_temp, 'true', ['prob_0', 'prob_1', 'prob_2'])\n",
    "        fig.suptitle(f\"H{h_int} - Calibration Curves\", fontsize=14, fontweight='bold', y=1.02)\n",
    "        plt.show()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Precision-Recall Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"PRECISION-RECALL CURVES\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for h_int, model_dict in state.best_model_by_horizon.items():\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"H{h_int} Precision-Recall\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    y_test = model_dict.get(\"y_test\")\n",
    "    y_proba = model_dict.get(\"y_proba\")\n",
    "    \n",
    "    if y_test is None or y_proba is None:\n",
    "        print(\"  No probability data available\")\n",
    "        continue\n",
    "    \n",
    "    try:\n",
    "        df_temp = pd.DataFrame({\n",
    "            'true': y_test,\n",
    "            'prob_0': y_proba[:, 0],\n",
    "            'prob_1': y_proba[:, 1] if y_proba.shape[1] > 1 else 0,\n",
    "            'prob_2': y_proba[:, 2] if y_proba.shape[1] > 2 else 0,\n",
    "        })\n",
    "        \n",
    "        fig = plot_precision_recall_curves(df_temp, 'true', ['prob_0', 'prob_1', 'prob_2'])\n",
    "        fig.suptitle(f\"H{h_int} - Precision-Recall Curves\", fontsize=14, fontweight='bold', y=1.02)\n",
    "        plt.show()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"ERROR ANALYSIS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for h_int, model_dict in state.best_model_by_horizon.items():\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"H{h_int} Error Analysis\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    y_test = model_dict.get(\"y_test\")\n",
    "    y_pred = model_dict.get(\"y_pred\")\n",
    "    y_proba = model_dict.get(\"y_proba\")\n",
    "    \n",
    "    if y_test is None or y_pred is None:\n",
    "        print(\"  No test data available\")\n",
    "        continue\n",
    "    \n",
    "    try:\n",
    "        # Calculate confidence\n",
    "        if y_proba is not None:\n",
    "            confidence = np.max(y_proba, axis=1)\n",
    "        else:\n",
    "            confidence = np.ones(len(y_pred))\n",
    "        \n",
    "        df_temp = pd.DataFrame({\n",
    "            '_true': y_test,\n",
    "            '_pred': y_pred,\n",
    "            '_conf': confidence,\n",
    "        })\n",
    "        \n",
    "        error_results = analyze_errors(df_temp, '_true', '_pred', '_conf')\n",
    "        \n",
    "        print(f\"\\n  Total Test Samples:     {error_results['total_samples']:,}\")\n",
    "        print(f\"  Total Errors:           {error_results['total_errors']:,} ({error_results['error_rate']:.2%})\")\n",
    "        print(f\"  High-Confidence Errors: {error_results['high_confidence_errors']:,} (conf >= 80%)\")\n",
    "        print(f\"  Mean Confidence (Correct): {error_results['mean_conf_correct']:.3f}\")\n",
    "        print(f\"  Mean Confidence (Errors):  {error_results['mean_conf_errors']:.3f}\")\n",
    "        \n",
    "        print(f\"\\n  Error Breakdown:\")\n",
    "        for error_type, count in error_results['error_type_counts'].items():\n",
    "            print(f\"    {error_type:25s}: {count}\")\n",
    "        \n",
    "        # Plot error analysis\n",
    "        fig = plot_error_analysis(df_temp, '_true', '_pred', '_conf')\n",
    "        fig.suptitle(f\"H{h_int} - Error Analysis\", fontsize=14, fontweight='bold', y=1.02)\n",
    "        plt.show()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Feature Importance (Top Predictive Words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"FEATURE IMPORTANCE - TOP PREDICTIVE WORDS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for h_int, model_dict in state.best_model_by_horizon.items():\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"H{h_int} Feature Importance\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    model = model_dict.get(\"model\")\n",
    "    vectorizer = model_dict.get(\"vectorizer\") or state.vectorizer\n",
    "    \n",
    "    if model is None:\n",
    "        print(\"  No model available\")\n",
    "        continue\n",
    "    \n",
    "    if vectorizer is None:\n",
    "        print(\"  No vectorizer available\")\n",
    "        continue\n",
    "    \n",
    "    try:\n",
    "        # Extract feature importance\n",
    "        importance_df = extract_feature_importance(model, vectorizer, top_n=20)\n",
    "        \n",
    "        if importance_df is not None and len(importance_df) > 0:\n",
    "            print(f\"\\n  Top 20 Features:\")\n",
    "            for _, row in importance_df.head(20).iterrows():\n",
    "                print(f\"    {row['feature']:30s} -> Class {int(row['class'])} (coef={row['coefficient']:.4f})\")\n",
    "            \n",
    "            # Plot feature importance\n",
    "            fig = plot_feature_importance(importance_df, top_n=15)\n",
    "            fig.suptitle(f\"H{h_int} - Top Predictive Words\", fontsize=14, fontweight='bold', y=1.02)\n",
    "            plt.show()\n",
    "        else:\n",
    "            print(\"  Could not extract feature importance\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"  Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. Color Change Prediction Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"COLOR CHANGE PREDICTION ANALYSIS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "def analyze_color_changes(df, horizon_suffix=''):\n",
    "    \"\"\"Analyze predicted color changes.\"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    # Get color columns\n",
    "    last_color_col = 'last_color' if 'last_color' in df.columns else 'Overall'\n",
    "    pred_color_col = f'pred_color{horizon_suffix}' if f'pred_color{horizon_suffix}' in df.columns else 'predicted_color'\n",
    "    \n",
    "    if last_color_col not in df.columns or pred_color_col not in df.columns:\n",
    "        return None\n",
    "    \n",
    "    df_valid = df[[last_color_col, pred_color_col]].dropna()\n",
    "    \n",
    "    # Calculate changes\n",
    "    df_valid['is_change'] = df_valid[last_color_col] != df_valid[pred_color_col]\n",
    "    \n",
    "    color_order = {'G': 0, 'Y': 1, 'R': 2}\n",
    "    df_valid['last_ord'] = df_valid[last_color_col].map(color_order)\n",
    "    df_valid['pred_ord'] = df_valid[pred_color_col].map(color_order)\n",
    "    \n",
    "    df_valid['is_downgrade'] = df_valid['pred_ord'] > df_valid['last_ord']\n",
    "    df_valid['is_upgrade'] = df_valid['pred_ord'] < df_valid['last_ord']\n",
    "    \n",
    "    results['total'] = len(df_valid)\n",
    "    results['changes'] = df_valid['is_change'].sum()\n",
    "    results['downgrades'] = df_valid['is_downgrade'].sum()\n",
    "    results['upgrades'] = df_valid['is_upgrade'].sum()\n",
    "    results['no_change'] = results['total'] - results['changes']\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Analyze for each horizon\n",
    "for h_int, pred_df in state.predictions_df_by_horizon.items():\n",
    "    if pred_df is None:\n",
    "        continue\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"H{h_int} - Color Change Analysis\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    results = analyze_color_changes(pred_df)\n",
    "    if results:\n",
    "        print(f\"\\n  Total Predictions:   {results['total']:,}\")\n",
    "        print(f\"  Predicted Changes:   {results['changes']:,} ({results['changes']/results['total']:.1%})\")\n",
    "        print(f\"    - Downgrades:      {results['downgrades']:,}\")\n",
    "        print(f\"    - Upgrades:        {results['upgrades']:,}\")\n",
    "        print(f\"  No Change:           {results['no_change']:,} ({results['no_change']/results['total']:.1%})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 10. Prediction Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"PREDICTION DASHBOARD\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Prepare data for reporting\n",
    "if state.predictions_df is not None:\n",
    "    report_df = state.predictions_df.copy()\n",
    "    \n",
    "    # Enrich for reporting\n",
    "    try:\n",
    "        enriched_report = enrich_for_reporting(report_df)\n",
    "        \n",
    "        # Generate summary tables\n",
    "        summaries = generate_summary_tables(enriched_report)\n",
    "        \n",
    "        print(\"\\nSummary Tables:\")\n",
    "        for name, summary_df in summaries.items():\n",
    "            print(f\"\\n{name}:\")\n",
    "            display(summary_df)\n",
    "        \n",
    "        # Generate dashboard\n",
    "        fig = plot_prediction_dashboard(enriched_report)\n",
    "        plt.show()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error generating dashboard: {e}\")\n",
    "else:\n",
    "    print(\"No predictions_df available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 11. GPT Justification Analysis\n",
    "\n",
    "This section demonstrates GPT-powered explanations for model predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"GPT JUSTIFICATION SETUP\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Create RAG object if not loaded\n",
    "if rag is None and conn is not None:\n",
    "    print(\"\\nCreating RAG object...\")\n",
    "    rag = ScoreCardRag(config=config, state=state, conn=conn)\n",
    "    print(\"RAG object created!\")\n",
    "elif rag is not None:\n",
    "    print(\"\\nRAG object already loaded!\")\n",
    "else:\n",
    "    print(\"\\nWARNING: Cannot create RAG - no connection available\")\n",
    "    print(\"Re-run load_state with reconnect=True and reload_embeddings=True\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to display GPT justification nicely\n",
    "def display_gpt_justification(sid_key, rag, state, show_context=True):\n",
    "    \"\"\"\n",
    "    Generate and display a GPT justification for a specific sid_key.\n",
    "    \"\"\"\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"GPT JUSTIFICATION FOR: {sid_key}\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Get the row from complete_df\n",
    "    df = state.complete_df\n",
    "    if df is None:\n",
    "        print(\"ERROR: complete_df not available\")\n",
    "        return\n",
    "    \n",
    "    row = df[df['sid_key'] == sid_key]\n",
    "    if row.empty:\n",
    "        print(f\"ERROR: sid_key '{sid_key}' not found\")\n",
    "        return\n",
    "    \n",
    "    row = row.iloc[0]\n",
    "    \n",
    "    # Display basic info\n",
    "    print(f\"\\n--- Basic Information ---\")\n",
    "    print(f\"SID: {row.get('SID', 'N/A')}\")\n",
    "    print(f\"Vendor: {row.get('Supplier_Name', row.get('LM_Vendor_ID', 'N/A'))}\")\n",
    "    print(f\"Program: {row.get('Program_Name', 'N/A')}\")\n",
    "    print(f\"Note Date: {row.get('Note_Year', 'N/A')}-{row.get('Note_Month', 'N/A')}\")\n",
    "    \n",
    "    # Display prediction\n",
    "    print(f\"\\n--- Model Prediction ---\")\n",
    "    print(f\"Predicted Color: {row.get('predicted_color', 'N/A')}\")\n",
    "    print(f\"Confidence: Green={row.get('prob_green', 0):.1%}, Yellow={row.get('prob_yellow', 0):.1%}, Red={row.get('prob_red', 0):.1%}\")\n",
    "    print(f\"Actual Color: {row.get('Overall', 'N/A')}\")\n",
    "    print(f\"Color History: {row.get('color_set', 'N/A')}\")\n",
    "    \n",
    "    # Display the note text\n",
    "    if show_context:\n",
    "        print(f\"\\n--- Scorecard Note ---\")\n",
    "        note_text = row.get('Scorecard_Note', row.get('pre_scrub_text', 'N/A'))\n",
    "        print(note_text[:1000] + \"...\" if len(str(note_text)) > 1000 else note_text)\n",
    "    \n",
    "    # Generate GPT justification\n",
    "    print(f\"\\n--- GPT Justification ---\")\n",
    "    print(\"Generating justification (this may take a moment)...\\n\")\n",
    "    \n",
    "    try:\n",
    "        # Get augmented history\n",
    "        history = rag.retrieve_augmented_history(sid_key)\n",
    "        \n",
    "        # Generate justification\n",
    "        rag.generate_justifications(sid_key, printer=True)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"ERROR generating justification: {e}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get sample sid_keys for GPT analysis\n",
    "print(\"=\" * 70)\n",
    "print(\"SAMPLE PREDICTIONS FOR GPT ANALYSIS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "if state.complete_df is not None:\n",
    "    df = state.complete_df\n",
    "    \n",
    "    # Find interesting cases\n",
    "    print(\"\\n1. High-confidence predictions (>80%):\")\n",
    "    if 'prob_green' in df.columns:\n",
    "        high_conf = df[\n",
    "            (df['prob_green'] > 0.8) | \n",
    "            (df['prob_yellow'] > 0.8) | \n",
    "            (df['prob_red'] > 0.8)\n",
    "        ].head(5)\n",
    "        if not high_conf.empty:\n",
    "            display(high_conf[['sid_key', 'SID', 'predicted_color', 'prob_green', 'prob_yellow', 'prob_red']].head())\n",
    "            sample_high_conf = high_conf['sid_key'].iloc[0]\n",
    "            print(f\"\\nSample high-confidence sid_key: {sample_high_conf}\")\n",
    "    \n",
    "    print(\"\\n2. Predicted color changes (downgrades):\")\n",
    "    if 'predicted_color' in df.columns and 'Overall' in df.columns:\n",
    "        downgrades = df[\n",
    "            ((df['Overall'] == 'G') & (df['predicted_color'].isin(['Y', 'R']))) |\n",
    "            ((df['Overall'] == 'Y') & (df['predicted_color'] == 'R'))\n",
    "        ].head(5)\n",
    "        if not downgrades.empty:\n",
    "            display(downgrades[['sid_key', 'SID', 'Overall', 'predicted_color', 'prob_green', 'prob_yellow', 'prob_red']].head())\n",
    "            sample_downgrade = downgrades['sid_key'].iloc[0]\n",
    "            print(f\"\\nSample downgrade sid_key: {sample_downgrade}\")\n",
    "    \n",
    "    print(\"\\n3. Recent predictions (for review):\")\n",
    "    recent = df.sort_values(['Note_Year', 'Note_Month'], ascending=False).head(5)\n",
    "    display(recent[['sid_key', 'SID', 'Note_Year', 'Note_Month', 'predicted_color', 'Overall']].head())\n",
    "else:\n",
    "    print(\"No complete_df available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate GPT justification for a specific sid_key\n",
    "# Replace with an actual sid_key from your data\n",
    "\n",
    "# Example: Use a sample from the data\n",
    "if state.complete_df is not None and rag is not None:\n",
    "    # Get a sample sid_key (you can change this to any specific one)\n",
    "    sample_sid_key = state.complete_df['sid_key'].iloc[0]\n",
    "    \n",
    "    print(f\"Generating GPT justification for: {sample_sid_key}\")\n",
    "    print(\"(Change sample_sid_key to analyze a different note)\\n\")\n",
    "    \n",
    "    display_gpt_justification(sample_sid_key, rag, state, show_context=True)\n",
    "else:\n",
    "    print(\"Cannot generate justification - missing data or RAG object\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 12. Batch GPT Justifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate justifications for multiple predictions\n",
    "# WARNING: This can be slow and consume API credits!\n",
    "\n",
    "def batch_generate_justifications(sid_keys, rag, state, max_count=5):\n",
    "    \"\"\"\n",
    "    Generate GPT justifications for multiple sid_keys.\n",
    "    \"\"\"\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"BATCH GPT JUSTIFICATIONS (max {max_count})\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    results = []\n",
    "    for i, sid_key in enumerate(sid_keys[:max_count]):\n",
    "        print(f\"\\n[{i+1}/{min(len(sid_keys), max_count)}] Processing: {sid_key}\")\n",
    "        try:\n",
    "            rag.generate_justifications(sid_key, printer=False)\n",
    "            results.append({'sid_key': sid_key, 'status': 'success'})\n",
    "            print(f\"  -> Success!\")\n",
    "        except Exception as e:\n",
    "            results.append({'sid_key': sid_key, 'status': 'error', 'error': str(e)})\n",
    "            print(f\"  -> Error: {e}\")\n",
    "    \n",
    "    print(f\"\\n\" + \"=\" * 70)\n",
    "    success_count = sum(1 for r in results if r['status'] == 'success')\n",
    "    print(f\"Completed: {success_count}/{len(results)} successful\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Example: Generate for top 5 high-confidence predictions\n",
    "# Uncomment to run:\n",
    "# if state.complete_df is not None and rag is not None:\n",
    "#     sample_keys = state.complete_df['sid_key'].head(5).tolist()\n",
    "#     batch_results = batch_generate_justifications(sample_keys, rag, state, max_count=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 13. Retrieve Stored Justifications from Elasticsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stored_justification(sid_key, conn, config):\n",
    "    \"\"\"\n",
    "    Retrieve a previously generated justification from Elasticsearch.\n",
    "    \"\"\"\n",
    "    es = conn.es_client\n",
    "    try:\n",
    "        result = es.get(index=config.rag_index, id=sid_key)\n",
    "        source = result['_source']\n",
    "        return {\n",
    "            'sid_key': sid_key,\n",
    "            'justification': source.get('justification', source.get('gpt_justification', '')),\n",
    "            'note': source.get('Scorecard_Note', ''),\n",
    "            'created_at': source.get('created_at', ''),\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {'sid_key': sid_key, 'error': str(e)}\n",
    "\n",
    "def display_stored_justification(sid_key, conn, config):\n",
    "    \"\"\"\n",
    "    Display a stored justification nicely.\n",
    "    \"\"\"\n",
    "    result = get_stored_justification(sid_key, conn, config)\n",
    "    \n",
    "    print(\"=\" * 70)\n",
    "    print(f\"STORED JUSTIFICATION: {sid_key}\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    if 'error' in result:\n",
    "        print(f\"Error: {result['error']}\")\n",
    "        return\n",
    "    \n",
    "    print(f\"\\nCreated: {result.get('created_at', 'Unknown')}\")\n",
    "    print(f\"\\n--- Note ---\")\n",
    "    print(result.get('note', 'N/A')[:500])\n",
    "    print(f\"\\n--- Justification ---\")\n",
    "    print(result.get('justification', 'No justification stored'))\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "# Example usage:\n",
    "# if conn is not None:\n",
    "#     display_stored_justification('000001.2024.06.000001', conn, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 14. Interactive Analysis\n",
    "\n",
    "Use the cells below for ad-hoc analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick data exploration\n",
    "print(\"Available DataFrames in state:\")\n",
    "for attr in ['enriched_df', 'predictions_df', 'complete_df', 'sid_df', 'details_df']:\n",
    "    df = getattr(state, attr, None)\n",
    "    if df is not None:\n",
    "        print(f\"  - state.{attr}: {df.shape}\")\n",
    "        print(f\"    Columns: {list(df.columns)[:5]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze a specific SID\n",
    "def analyze_sid(sid, state):\n",
    "    \"\"\"Show all predictions for a specific SID.\"\"\"\n",
    "    df = state.complete_df\n",
    "    if df is None:\n",
    "        print(\"No data available\")\n",
    "        return\n",
    "    \n",
    "    sid_data = df[df['SID'] == sid].sort_values(['Note_Year', 'Note_Month'])\n",
    "    \n",
    "    if sid_data.empty:\n",
    "        print(f\"No data for SID {sid}\")\n",
    "        return\n",
    "    \n",
    "    print(f\"\\nSID {sid} - {len(sid_data)} notes\")\n",
    "    print(f\"Vendor: {sid_data.iloc[0].get('Supplier_Name', 'N/A')}\")\n",
    "    print(f\"Program: {sid_data.iloc[0].get('Program_Name', 'N/A')}\")\n",
    "    \n",
    "    cols = ['sid_key', 'Note_Year', 'Note_Month', 'Overall', 'predicted_color', 'prob_green', 'prob_yellow', 'prob_red']\n",
    "    available_cols = [c for c in cols if c in sid_data.columns]\n",
    "    display(sid_data[available_cols])\n",
    "\n",
    "# Example:\n",
    "# analyze_sid(1, state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom GPT analysis cell - modify sid_key as needed\n",
    "# sid_key = \"000001.2024.01.000001\"  # Replace with actual sid_key\n",
    "# display_gpt_justification(sid_key, rag, state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 15. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"ANALYSIS SESSION SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\\nData Loaded:\")\n",
    "if state.enriched_df is not None:\n",
    "    print(f\"  - Enriched notes: {len(state.enriched_df):,}\")\n",
    "if state.predictions_df is not None:\n",
    "    print(f\"  - Predictions: {len(state.predictions_df):,}\")\n",
    "if state.complete_df is not None:\n",
    "    print(f\"  - Complete dataset: {len(state.complete_df):,}\")\n",
    "\n",
    "print(f\"\\nModels:\")\n",
    "for h, key in state.best_model_key_by_horizon.items():\n",
    "    print(f\"  - H{h}: {key[:50]}...\")\n",
    "\n",
    "print(f\"\\nConnections:\")\n",
    "print(f\"  - Elasticsearch: {'Connected' if conn and conn.es_client else 'Not connected'}\")\n",
    "print(f\"  - GPT Client: {'Available' if conn and conn.gpt_client else 'Not available'}\")\n",
    "print(f\"  - RAG: {'Ready' if rag else 'Not initialized'}\")\n",
    "\n",
    "print(f\"\\n\" + \"=\" * 70)\n",
    "print(\"Analysis complete!\")\n",
    "print(\"=\" * 70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
